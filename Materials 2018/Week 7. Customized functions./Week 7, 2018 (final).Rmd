---
title: "R wizardry course week 7, 2018"
author: "Oscar Montoya"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


```{r}
rm(list=ls(all=T))
```


```{r}
#setwd("/home/oscar/Dropbox/R wizardry TA")
data <- read.csv("/home/oscar/MEGAsync/R Wizardry/Materials 2018/Datasets/SimDataTrend.csv")
```


# Week 7 content. Customized functions.

7.1 General structure of a function
7.2 Creating a custom function: Standard Error
7.3 Passing custom functions to other functions: Aggregate
7.4 Coding a 'robust' custom function: What about NAs?
7.5 Functions for complex analyses: Mixing FOR loops and FUNCTIONS
7.6 Looping across complex functions/analyses
7.7 Plotting the above results


## 7.1.1 General structure of a function

```{r}
?function
help("function")
```

FUN <- function(ARGUMENT1 = object1, ARGUMENT2 = object2, ... )
{
  ARGUMENT1 () = FIRST THING TO DO
  ARGUMENT 2 () = SECOND THING TO DO 
  ... = OPTIONAL ARGUMENTS, means "expect more instructions"
  
  return(THE ITEM/SOLUTION/OBJECT/VALUE OF INTEREST)
}


To call and execute the function, type the name of the function and pass the parameters/object of interest to the correspondent arguments. The objects don't need to be preceeded by the name of the argument. However, if you decide not to use the ARGUMENT name, make sure to add the objects in the same order used when the function was created. For example, FUN(object1, object2) will return a different result than FUN(object2, object1), but FUN(ARGUMENT1 = object1, ARGUMENT2 = object2) and FUN(ARGUMENT2 = object2, ARGUMENT1 = object1) will return the same result.


## 7.2 Creating a custom function: Standard Error

By definition, SE is the sd of the sample-mean's divided by the sqrt of the sample size.
```{r}
?sqrt
?sd
?length
```


The following two functions are equivalent and will return the same answer:

Option 1

The code below creates a standard error (SE) function that can be used with our dataset.
```{r}
SE <- function(x){
  sdt_err <- sd(x)/sqrt(length(x))
  return(sdt_err)
}

SE

head(data)
SE(data$biomass)
```

Option 2

The code below returns both the SE and the MEAN of mydata
```{r}
SE_mean <- function(mydata){
  sdt_err <- sd(mydata)/sqrt(length(mydata))
  return(c(sdt_err, mean(mydata)))
}

SE_mean(data$biomass)
```


## 7.3 Passing custom functions to other functions: Aggregate

```{r}
?aggregate

aggregate(biomass ~ geno + nutrient, data, SE)
aggregate(biomass ~ geno + nutrient, data, mean)
```


## 7.4 Coding a 'robust' custom function: What about NAs?

Let's introduce missing data into SimData

```{r}
?sample

# Create a new object to avoid iverwritting your data
temp <- data
set.seed(555)
temp[sample(1:, 1000),] <- NA # this introduces 1000 random NAs

temp[is.na(temp)]
#View(temp)
```


Now, run SE on the new SimData with the introduced NAs
```{r}
SE(temp)
```


We need to add some argument to handle missing data (NA)
```{r}
SE <- function(x, ...){
  sdt_err <- sd(x, ...)/sqrt(length(x[!is.na(x)]))
  return(sdt_err)
}

SE(temp$biomass, na.rm = TRUE)
```


## 7.5 Functions for complex analyses: Mixing FOR loops and FUNCTIONS


Power Analyses
Help us to determine a sample size required to detect an effect of a given size with a given degree of confidence. 

Let's create Markov Chain Monte Carlo (MCMC) simulation (a Bayesian-based method).

Knowing the difference in mean biomass between tretments and controls, we can compute the probability of observing a value as large as we did, referred to as a p-value

First, let's take a look at t-tests in R
```{r}

# t test has assumption of normality

shapiro.test(data$biomass) # Doesn't deal with more than 5,000 observations. Big datasets increase the detection of even small deviations from normality, which for statistical purposes might not be that critical.


# Base R

par(mfrow = c(1, 2)) # Divide plotting pannel into two columns
qqnorm(data$biomass[data$nutrient == 'Treatment']); qqline(data$biomass[data$nutrient == 'Treatment'])
qqnorm(data$biomass[data$nutrient == 'Control']); qqline(data$biomass[data$nutrient == 'Control'])

# ggplot2
#detach("package:ggplot2", unload=TRUE)
ggplot2::ggplot(data, aes(biomass)) +
  geom_density()


t_test_results <- t.test(data$biomass[data$nutrient == 'Treatment'], data$biomass[data$nutrient == 'Control'])

t_test_results # p<0.05 so there's statistical difference.


# Access t_test_results object
str(t_test_results)

t_test_results


# Calculate the mean difference between treatment and control groups
difference <- mean(data$biomass[data$nutrient=='Treatment']) -
  mean(data$biomass[data$nutrient=='Control'])

print(difference)

t_test_results
```


```{r}
?rbind
?order
?diff # calculates the differences between all consecutive values of a vector

power <- function(x1, x2, nsamp, draws=1000) # 1000 = bootstraps
  #x1=treatment, x2, control, nsamp=how many samples at the time
{
  boot <- NULL
  for(i in 1:draws)
    {
    treat <- sample(x1, nsamp) # treatments
    contr <- sample(x2, nsamp) # controls
    t <- t.test(treat, contr) #(mean(treat)-mean(contr))/sqrt(var(treat)/treat + var(contr)/contr)
    
    results <- c(diff(t$estimate), t$p.value) # t$estimate = t-value
    boot <- rbind(boot, results)
  }
  boot <- boot[order(boot[ , 1]), ] #return the iterations ordered based on column 1
  test <- as.vector(c(nsamp,
                      boot[round(draws * 0.5), 1],
                      boot[round(draws * 0.5), 2]))
  return(test)

}

set.seed(10)
power(x1 = data$biomass[data$nutrient=='Treatment'],
      x2 = data$biomass[data$nutrient=='Control'],
      nsamp = 20, draws = 1000)




#With 10 samples, there is no significant difference in the biomass of treatments and controls (p > 0.5). In this test we didn't include other factors influencing the experiment, like blocks or days, for example.

#nsam =10   t.estimate= -1.2769380  p-value= 0.2262022

```




## 7.6 Looping across complex functions/analyses

```{r}
nsim <- c(2, 5, 10, 15, 20, 25, 30, 50, 100)

set.seed(20)

results <- NULL

for (i in 1:length(nsim))
{
  N <- nsim[i]
  draws <- 1000
  treat <- data$biomass[data$nutrient=='Treatment']
  contr <- data$biomass[data$nutrient=='Control']
  test <- power(treat, contr, N, draws)
  results <- rbind(results, test)
}

results
#It seems we can get statistical significance with a minimum of 15 samples.
```



## 7.7 Plotting the above results

```{r}

par(mfrow = c(1, 1))

plot(log(results[ , 1]), results[ , 3], xaxt = "n", xlab = "Samples", 
     ylab = "p-value", pch=20)
axis(1, at=log(nsim), labels = nsim)
abline(h=0.05, col="red", lty = 2)

#We can get statistical significance from 15 samples and above. The more samples the merrier but we all know that is unrealistic.
```



